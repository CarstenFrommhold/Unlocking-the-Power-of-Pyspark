{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupied-shower",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Let's get started\n",
    "\n",
    "<!-- ![](https://www.datadrivers.de/wp-content/uploads/2020/10/Weblogo_Datadrivers_GmbH.svg) -->\n",
    "\n",
    "What could be better than starting with a blank slate? There we go...\n",
    "\n",
    "**Note:** This notebook is aimed at Data Scientists and/or Data Engineers and treats the technical concepts superficially. It focuses on **first steps in data processing** and does not cover execution on graphs, model training and streaming/deployment purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3202c5c-2e94-4164-b405-7795a9b6a985",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-smith",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "import random \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-science",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialize Spark on localhost\n",
    "\n",
    "Home, sweet home. üè† \n",
    "\n",
    "Before we move onto a real cluster, for example in the cloud, let's simulate this for now. This offers the chance to familiarize ourself with the key concepts, before moving ahead.\n",
    "\n",
    "When a Spark cluster is started on localhost, a local instance of Spark is launched on the machine. The Spark driver program acts as the master node, and one or more Spark executors are launched as worker nodes.\n",
    "\n",
    "The SparkContext is the entry point for low-level APIs of Spark, and it represents the connection to a Spark cluster. A SparkSession, on the other hand, is a higher-level API introduced in Spark 2.0 that provides a single entry point to create and manage Spark functionality, including SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-saver",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session: SparkSession = SparkSession.builder.master(\"local\").appName(\"Local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-defense",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-irish",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-watson",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start from scratch: RDDs\n",
    "\n",
    "RDDs (Resilient Distributed Datasets) are fundamental data structures in Spark that enable distributed processing of by allowing data to be stored in memory across multiple machines and processed in parallel. They are immutable and fault-tolerant, making them ideal for distributed computing.\n",
    "\n",
    "What happens when an RDD is created?\n",
    "\n",
    "* The driver program (i.e., the program that creates the RDD) creates a logical representation of the RDD and sends it to the Spark cluster.\n",
    "* The Spark cluster divides the RDD into smaller **partitions**, which are distributed across the nodes in the cluster.\n",
    "\n",
    "The decision on how to split an RDD into partitions is made by Spark at the time the RDD is created. The number of partitions and the partitioning scheme are determined based on the size of the RDD and the available resources in the cluster. For small samples, it is likely that the entire RDD can fit in memory on each node in the cluster, and therefore the Spark cluster may choose to create only one partition for the entire RDD. \n",
    "\n",
    "By default, Spark creates one partition for every block of data in the RDD, where a block is typically 128MB by default, but this can be configured.\n",
    "\n",
    "<img src=\"img/rdd_distribution.png\" width=\"50%\">\n",
    "\n",
    "Having multiple partitions allows Spark to parallelize the processing of data, which can improve performance by allowing multiple nodes in the cluster to work on different parts of the data simultaneously. This enables Spark to scale to handle large datasets by breaking them down into smaller chunks that can be processed in parallel across multiple nodes in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-procedure",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms = [random.randint(0,10) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-anchor",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_rdd = sc.parallelize(randoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-aquarium",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-lyric",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformations, actions and lazy excecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-grade",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After covering the first concept of *parallelization*, we have three more buzzwords to deal with.\n",
    "\n",
    "* Transformations\n",
    "* Actions\n",
    "* Lazy excecution\n",
    "\n",
    "*Transformations* describe operations on the data like function mapping, filtering etc.\n",
    "\n",
    "*Actions* are operations which use the result of data processing and output them, e.g. showing or printing a result, counting the number of observations or writing the result to an HDD. They process the data in that sence that the chain of **transformations needs to be excecuted** at this point.\n",
    "\n",
    "Spark transformations are *lazy*. This means that they are executed only when it is necessary. This is generally the case with an action. All transformations are stored in an *excecution-plan* (viewable in a so-called DAG (*Directed Acyclic Graph*). This gives rise to **optimization opportunities** and **fault tolerance**. \n",
    "\n",
    "One example of an action is *take*, which forces Spark to to take the first n entries. In that case, the driver node needs to collect transformed data from the RDDs on the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-parade",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd0c8d-f3cb-4c5a-aad3-b3272a64c9b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_then_sqare(x):\n",
    "    time.sleep(2)\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-lawsuit",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_squared = randoms_rdd.map(lambda x: wait_then_sqare(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf56bb7-ad70-4042-96e0-51ef61efaddf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wait.. It should take a nap? Why did the execution only took a millisecond?  \n",
    "\n",
    "Because of the lazyness... No one asked for a result yet.\n",
    "\n",
    "So, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-flexibility",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_squared.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-radiation",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DataFrames...\n",
    "\n",
    "A Spark DataFrame is a distributed collection of data organized into named columns. It is similar to a table in a relational database. Under the hood, Spark DataFrames are built on top of the Spark SQL engine, which is responsible for executing SQL queries, performing optimizations, and managing data serialization and deserialization. It provides a high-level API for working with structured and semi-structured data, making it easy to manipulate large datasets. Spark DataFrames are implemented using RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-compensation",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ...from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-links",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_scratch = spark_session.createDataFrame([\n",
    "    {\"random\": random.randint(0, 10)}  for _ in range(100)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-listing",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_scratch.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-audio",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_scratch.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-cookbook",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ... via RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-signature",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms = [random.randint(0,10) for _ in range(1000)]\n",
    "randoms_rdd = sc.parallelize(randoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-promise",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-deadline",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the toDF() function expects an input of shape (n, 1) and not (1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-lucas",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_rdd = randoms_rdd.map(lambda x:[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-barrel",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-numbers",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_rdd = randoms_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-cotton",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18caa42-09b9-4d17-946e-98000f84f2ab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In that case, it is better to define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-twist",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"random\", IntegerType(), False)])\n",
    "\n",
    "df_from_rdd = randoms_rdd.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-syntax",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_rdd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-vermont",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-highland",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ... or from a data source\n",
    "\n",
    "In the real world, it's probably more likely to fetch a data source and read it in. \n",
    "\n",
    "When reading a CSV file in PySpark, the file is read in chunks and does not need to be fully opened in memory by the driver. This allows PySpark to efficiently process large CSV files: Each partition is read by a separate worker node in the PySpark cluster, allowing for parallel processing.\n",
    "\n",
    "In contrast, when reading an Parquet file in PySpark, the data is read in a columnar format, which allows for more efficient processing. Parquet files are also highly compressed, which further reduces the data size and makes it more efficient to transfer across the network. \n",
    "\n",
    "Also, PySpark provides a JDBC data source, which allows one to read data from any database that supports JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5463169-e0f2-4277-bb33-3057db865800",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "UPDATE: bool = False\n",
    "\n",
    "if UPDATE:\n",
    "    current_season = 2223\n",
    "    matches = f'https://www.football-data.co.uk/mmz4281/{current_season}/D1.csv'\n",
    "    os.system(f\"curl -o data/bundesliga_.csv {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298853c-a0c0-4938-a958-18fc124c1ff6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if UPDATE:\n",
    "    os.system(\"cut -d ',' -f 1,2,3,4,5,6,7 data/bundesliga_.csv > data/bundesliga.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-sending",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_csv = spark_session.read.csv(\"data/bundesliga.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-morning",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_csv.show(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-praise",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-tumor",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build sample data as parquet\n",
    "\n",
    "# df = df_from_csv.withColumn(\"FTHG\", F.col(\"FTHG\").cast(IntegerType()))\n",
    "# df = df.withColumn(\"FTAG\", F.col(\"FTAG\").cast(IntegerType()))\n",
    "# df = df.withColumnRenamed(\"Div\", \"Matchday\")\n",
    "# df.coalesce(1).write.parquet(\"data/bundesliga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-jungle",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_parquet = spark_session.read.parquet(\"data/bundesliga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-multimedia",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-solid",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_from_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1642d-c747-49cf-877e-923bd56f804c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hands On: An example pipeline on a DataFrame\n",
    "\n",
    "We take a look at the German Bundesliga. ‚öΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10173aa9-bcbf-479b-9071-e3b226c1bdc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark_session.read.parquet(\"data/bundesliga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e0146-fa69-4c34-ad4a-e50ed04cdfae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d77bae-42c7-4fa3-a13e-c24d64feca8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"GoalDelta\", F.col(\"FTHG\") - F.col(\"FTAG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db459da-5e01-4828-8fba-bb5ee716324b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"PointsHome\", F.lit(1))\n",
    "df = df.withColumn(\"PointsHome\", F.when(\n",
    "    F.col(\"GoalDelta\") > 0, F.lit(3)).otherwise(F.col(\"PointsHome\")))\n",
    "df = df.withColumn(\"PointsHome\", F.when(\n",
    "    F.col(\"GoalDelta\") < 0, F.lit(0)).otherwise(F.col(\"PointsHome\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87becc-042e-4f89-b8d7-568f54a7440a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"PointsAway\", F.lit(1))\n",
    "df = df.withColumn(\"PointsAway\", F.when(\n",
    "    F.col(\"GoalDelta\") < 0, F.lit(3)).otherwise(F.col(\"PointsAway\")))\n",
    "df = df.withColumn(\"PointsAway\", F.when(\n",
    "    F.col(\"GoalDelta\") > 0, F.lit(0)).otherwise(F.col(\"PointsAway\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19640d-b06a-48d9-bc0f-de54d5cc15e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_home = df.select(\"HomeTeam\", \"FTHG\", \"FTAG\", \"PointsHome\").\\\n",
    "withColumnRenamed(\"HomeTeam\", \"Team\").\\\n",
    "withColumnRenamed(\"FTHG\", \"Goals\").\\\n",
    "withColumnRenamed(\"FTAG\", \"GoalsAgainst\").\\\n",
    "withColumnRenamed(\"PointsHome\", \"Points\")\n",
    "\n",
    "# An Alternative to avoid all this .withColumnRenamed-Calls: \n",
    "# new_columns = [\"Team\",\"Goals\",\"GoalsAgainst\", \"Points\"]\n",
    "# df_home = df.select(\"HomeTeam\", \"FTHG\", \"FTAG\", \"PointsHome\").toDF(*new_columns)\n",
    "\n",
    "df_away = df.select(\"AwayTeam\", \"FTHG\", \"FTAG\", \"PointsAway\").\\\n",
    "withColumnRenamed(\"AwayTeam\", \"Team\").\\\n",
    "withColumnRenamed(\"FTHG\", \"GoalsAgainst\").\\\n",
    "withColumnRenamed(\"FTAG\", \"Goals\").\\\n",
    "withColumnRenamed(\"PointsAway\", \"Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0eda9-c55b-4412-bd25-0712298a7da7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Select Statement ensures a consistent order of the DataFrames to be merged.\n",
    "cols = [\"Team\", \"Goals\" , \"GoalsAgainst\", \"Points\"]\n",
    "df_home = df_home.select(*cols)\n",
    "df_away = df_away.select(*cols)\n",
    "\n",
    "df = df_home.union(df_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ff1a5-354d-4af9-8e13-da846c94cdb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.groupBy(\"Team\").sum()\n",
    "df = df.withColumnRenamed(\"sum(Goals)\", \"Goals\").\\\n",
    "withColumnRenamed(\"sum(GoalsAgainst)\", \"GoalsAgainst\").\\\n",
    "withColumnRenamed(\"sum(Points)\", \"Points\")\n",
    "\n",
    "df = df.withColumn(\"GoalDifference\", F.col(\"Goals\") - F.col(\"GoalsAgainst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1708106-d477-43cd-9b5c-fb2034721eb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Let's sort by points and goal difference to get the table...\n",
    "\n",
    "Note: Sorting large datasets on a Spark cluster can be a resource-intensive operation. Sorting requires shuffling of data across the cluster. So avoid it where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84598846-de66-4ca6-bd50-32d884998841",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.sort(\"Points\", \"GoalDifference\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f466157-3c57-4e0d-a0df-dd592be1c1de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Place\", F.monotonically_increasing_id() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a37f36-e8a3-474c-abcc-1d7a98ddcb56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before the plan on a dataframe is executed, it is analyzed internally by the *Catalyst Optimizer* and the execution is then executed in an optimized manner. This leads to further **speed performance advantages.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7175f2-c550-4f71-bd01-e53fa497f989",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830bf9e-df04-4ea7-bc2e-e1190ff712fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3aa64-d6f9-4aeb-b50e-ecfd75a451b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Here we are.\n",
    "\n",
    "You might argue that this would be way more easy via SQL. In this case,  you are probably right.  \n",
    "Note that this acts as a demo. Also, here we have the chance to test each step individually. \n",
    "This is a huge advantage when developing a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c084d0-392c-4430-be6b-fe197eeed6e4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "It is possible to use SQL, even in an intermediate step. \n",
    "\n",
    "```python\n",
    "df.registerTempTable(\"my_table_name\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM my_table_name\n",
    "WHERE ...\n",
    "\"\"\"\n",
    "\n",
    "df_queried = spark_session.sql(query)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d1d69-492a-4cf7-925e-c01016bb2312",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### User defined functions\n",
    "\n",
    "It is also possible to define a user defined function and apply it to the DataFrame. Note that in ETL processes one should try to avoid udfs and to solve the problem via native spark functions instead. The reason is that pure Python acts as a **blackbox** for SparkSQL and will **not be optimized**. Also, data might need to be unnecessarily shuffled between the worker nodes and the driver. The syntax is as follows:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.udf import UserDefinedFunction\n",
    "\n",
    "def my_user_defined_square_function(x):\n",
    "    # square via pure Python\n",
    "    return x ** 2\n",
    "    \n",
    "my_udf = UserDefinedFunction(lambda z: my_user_defined_square_function(z), FloatType())\n",
    "\n",
    "df = df.withColumn(\"ResultColumn\", my_udf(F.col(\"Column\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-insert",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pitfalls with lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-tuesday",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Don't\n",
    "\n",
    "In an ideal world, the save method of the end of a script which marks the only action. In real world applications, however, it might be the case, that there occure multiple actions. This might be due to debugging purposes. Or take another example: Your application should react to the amount of observations in a DataFrame. Then, you need to apply the action *count*.\n",
    "\n",
    "The problem with this is that the pipeline is now triggered twice. First, up to the first action. However, the intermediate result is not automatically cached. If you continue to work on the DataFrames, the DAG is expanded and **fully executed** again with the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-receptor",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark_session.read.parquet(\"data/bundesliga\")\n",
    "\n",
    "# First transformation\n",
    "df = df.withColumn(\"GoalDelta\", F.col(\"FTHG\") - F.col(\"FTAG\"))\n",
    "\n",
    "n = df.count()\n",
    "\n",
    "if n < 9*5:\n",
    "    print(\"It is to early to judge teams performances.\")\n",
    "else:\n",
    "    print(\"Dont hide the truth.\")\n",
    "\n",
    "# Further transformations\n",
    "df = df.withColumn(\"Result\", F.lit(\"D\"))\n",
    "df = df.withColumn(\"Result\", F.when(\n",
    "    F.col(\"GoalDelta\") > 0, F.lit(\"H\")).otherwise(F.col(\"Result\")))\n",
    "df = df.withColumn(\"Result\", F.when(\n",
    "    F.col(\"GoalDelta\") < 0, F.lit(\"A\")).otherwise(F.col(\"Result\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-radar",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Better: Persist! \n",
    "\n",
    "There is a solution: you can persist and save the DataFrame on the Hard Drive and/or memory to a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-diesel",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark_session.read.parquet(\"data/bundesliga\")\n",
    "\n",
    "# First transformation\n",
    "df = df.withColumn(\"GoalDelta\", F.col(\"FTHG\") - F.col(\"FTAG\"))\n",
    "\n",
    "# Force a persist\n",
    "# Note that this does not happen immediately, since persist() is not an action itself!\n",
    "df = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "n = df.count()\n",
    "\n",
    "if n < 9*5:\n",
    "    print(\"It is to early to judge teams performances.\")\n",
    "else:\n",
    "    print(\"Dont hide the truth.\")\n",
    "\n",
    "# Further transformations\n",
    "df = df.withColumn(\"Result\", F.lit(\"D\"))\n",
    "df = df.withColumn(\"Result\", F.when(\n",
    "    F.col(\"GoalDelta\") > 0, F.lit(\"H\")).otherwise(F.col(\"Result\")))\n",
    "df = df.withColumn(\"Result\", F.when(\n",
    "    F.col(\"GoalDelta\") < 0, F.lit(\"A\")).otherwise(F.col(\"Result\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5fca9-c607-468b-8cc1-25fdfbf3312a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Some other pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-boxing",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Case sensitivity\n",
    "\n",
    "Note that different columns with same name can coexist in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-poultry",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame([{\"COL\": 1, \"col\": 2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-nitrogen",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-stereo",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Duplicated Column Names\n",
    "\n",
    "As long as no transformations are made on them, also columns with same case-sensitive names can coexist in a dataframe. This can be an issue when joining two dataframes where one has the same columns in both.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-smile",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"col\", \"COL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-genetics",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-print",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df.select(\"COL\").show() # will raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b86244-8621-45f6-95ec-8ac65f8cc273",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wrap-Up\n",
    "\n",
    "* We simulated the spark environment locally\n",
    "* RDDs are the basic data structure of Spark. They are immutable and distributed across the cluster.\n",
    "* DataFrames are a higher level abstraction of RDDs. They are also immutable and distributed across the cluster. They are also optimized for SQL-like operations.\n",
    "* A schema consists of a list of columns with their names and types as well as the nullability.\n",
    "* We distinguish between transformations and actions. Transformations are lazy and only executed when an action is called.\n",
    "* The so-called Catalyst Optimizer optimizes the execution plan of a DataFrame.\n",
    "* If several actions are called, persistence should be applied in the meantime.\n",
    "* The syntax sometimes seems a bit hard to get used to, due to the Scala and Java presences.\n",
    "* The tooling is very valuable. Especially when building large software and ETL processes.\n",
    "* Individual steps can be tested and reused in comparison to a pure SQL-based script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-daily",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congrats!\n",
    "\n",
    "You have made your first steps with Pyspark.  \n",
    "üöÄüöÄüöÄ\n",
    "\n",
    "At the end: Stop the local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-congo",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135d3a8-4269-4333-af9b-e97388f61536",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
