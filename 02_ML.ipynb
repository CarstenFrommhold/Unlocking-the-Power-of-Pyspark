{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c7d155",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a model\n",
    "\n",
    "With the knowledge of some Spark and PySpark basics, one can move on to machine learning utilities: Spark ML (*) \n",
    "\n",
    "Here, we take a simple dataset and fit a regressor. This does not claim to be a complete data science process. More important is to get to know the analogies and differences to known frameworks such as scikit-learn.\n",
    "\n",
    "(*) Often, one hears the term *MLLib*. This usually refers to the *ML* library by Spark. Strictly speaking, these are two different things. Spark MLlib is the older of the two and is applied directly to RDDs. Spark ML, on the other hand, is built on top of DataFrames and provides a higher-level API that abstracts away some of the low-level details of building and tuning machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38ffc0-5e0b-4f51-bf99-f4a5df9a163c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ceda32-eacb-4026-93cf-f0ac54b0ed82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session: SparkSession = SparkSession.builder.master(\"local\").appName(\"Local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795bb1f-5d2b-4271-a784-0adc7ae0ec51",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d624ceb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Iris, penguins, AirBnBs, Wines etc. You already know them by heart?   \n",
    "\n",
    "Well, then let's take a look at a more business specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65fcd3-f55b-4f1e-a7bd-2947d1153a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark_session.read.csv(\"data/mocked_customer_data.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae009232",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## EDA\n",
    "\n",
    "Performing EDA using PySpark has its pros and cons. On the one hand, PySpark supports various statistical functions that can help you calculate summary statistics, identify outliers, and explore relationships between variables. This can be particularly useful when dealing with very large datasets that cannot be easily processed using other tools.\n",
    "\n",
    "On the other hand, PySpark does not support plotting and visualization, which can be an important part of the EDA process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b121295",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1acda1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75d6c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd379767",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e040842",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928bfc5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mean = df.select([mean(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77f7a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983a236",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_std = df.select([stddev(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517224c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd908a49",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build a model...\n",
    "\n",
    "Let's build a simple classification model which aims to predict the *target*.\n",
    "\n",
    "But first, let's clarify [terminology](https://spark.apache.org/docs/latest/ml-pipeline.html#example-estimator-transformer-and-param) for a moment:\n",
    "\n",
    "* We already know **DataFrames**.\n",
    "* **Transformers** can transform one DataFrame to another DataFrame. An ML Transformer transforms a DataFrame with features to DataFrame with predictions.\n",
    "* **Estimators** create Transformers via Fitting on a DataFrame.\n",
    "* A **Pipeline** chains Transformers and Estimators to create a Pipeline workflow. But they are not covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65219d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"n1\", \"n2\", \"n3\", \"n4_1\", \"n4_2\", \"n4_3\"]\n",
    "TARGET = \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e1760",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most machine learning algorithms in Spark require input data to be in vector format. PySpark's VectorAssembler is a utility that allows you to combine multiple columns of a PySpark DataFrame into a single vector column. The resulting vector column is then used as an input to machine learning algorithms in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6577d6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURES,\n",
    "    outputCol=\"features_vec\"\n",
    ")\n",
    "df_prep = assembler.transform(df).select(\"features_vec\", TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbfdc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prep.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59877a4e-08dd-401e-9269-e203c7c253ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prep.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3283a60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df_prep.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db878f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Allright. Let's fit the baseline model.\n",
    "\n",
    "The syntax is very similar to sklearn. But, as mentioned above, there is a distingtion between Estimators and Transformers. So, the fitting process returns a Transformer object (in contrast to sklearn, where the object fitted object is able to make predictions directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa469e4a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=TARGET, featuresCol=\"features_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122b01d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = lr.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531a125",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The question whether and how the training process can now be parallelized depends on the model architecture. A knn clustering, for example, can be easily parallelized. An xgboost method, on the other hand, is somewhat more complex. Since the training process is sequential, only the individual steps can be parallelized here. The way in which the calculation is optimized differs in each case. This also applies to the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7deec7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c2014",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ... test it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78897a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed44365-d1cc-4f51-a804-68a1a5cce61b",
   "metadata": {},
   "source": [
    "Let's test the goodness of fit via \n",
    "* Accuarcy\n",
    "* F1 Score \n",
    "* Area under the curve\n",
    "\n",
    "Note that neither BinaryClassificationEvaluator nor MulticlassClassificationEvaluator can calculate all metrics on their own. We need to use both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50abe0-ee9e-4da3-a83c-2ba1f8553084",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=TARGET, \n",
    "                                              predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(df_pred, {evaluator.metricName: 'accuracy'})\n",
    "f1 = evaluator.evaluate(df_pred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}\")\n",
    "print(f\"f1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81e829-53e6-44c5-b716-66144925950c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=TARGET, \n",
    "                                          rawPredictionCol='rawPrediction')\n",
    "\n",
    "auc = evaluator.evaluate(df_pred, {evaluator.metricName: 'areaUnderROC'})\n",
    "\n",
    "print(f\"AUC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbc5be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ... and persist it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc04d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So here we are. We've got a model in our hands. Hooray.\n",
    "\n",
    "For sure, we don't want to train a model again and again but persist it. The Transformer object comes with an built in function to do so. One can persist the model on localhost or any blob storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8122c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd41c7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"model/my_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931f30e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(path):\n",
    "    os.system(f'rm -r {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cebc0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d397d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, in any production environment, one can load the model to work with it. Note that this could also happen in Scala or Java directly, because the serialization was not done via pickle but is language-agnostic.\n",
    "Again, one faces some unintuitive syntax, because the ```load()``` method is implemented in a Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea3947",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_loaded = LogisticRegressionModel.load(\"model/my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d31d50",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Chain in a pipeline\n",
    "\n",
    "There is also the possibility to capture these two steps directly in one object. Pipelines form a construct in which several Transformers and Estimators can be chained together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539232c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURES,\n",
    "    outputCol=\"features_vec\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(labelCol=TARGET, featuresCol=\"features_vec\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75204df9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case, the train-test-split has to be applied directly on the original DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd135c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_direct, df_test_direct = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52c489",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81b7ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = model.transform(df_test_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba11f84",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6aa2ab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "Note that the example above is for education purposes. In real world problems, one might try out different hyperparameter setups as well as different models to reach the best one. Also, cross validation should be applied if possible to acchieve more stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4772e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURES,\n",
    "    outputCol=\"features_vec\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(labelCol=TARGET, featuresCol=\"features_vec\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=TARGET, \n",
    "                                          rawPredictionCol='rawPrediction',\n",
    "                                          metricName=\"areaUnderROC\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "parameter_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=parameter_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2\n",
    ")\n",
    "\n",
    "cross_val_model = crossval.fit(df_train_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a3dc5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEEP_DIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ccc47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEEP_DIVE: \n",
    "    print(cross_val_model.bestModel.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4d190",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEEP_DIVE:\n",
    "    print(cross_val_model.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4507c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Challenges in a fitting process.\n",
    "\n",
    "Real data sets usually require some pre-processing before fitting is possible.\n",
    "\n",
    "* For instance, not every ML model is able to handle missing values. One needs to drop oder impute missing values first. \n",
    "* Also, some models are sensitive to different scales in the expressions of numerical values, so one needs to standardize.\n",
    "* Speaking about numerical values, for many models categorical data have to be encoded first. \n",
    "\n",
    "For those cases, there exist Transformers as well. One should integrate them in the modelling process, best directly into a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ef318-9e4c-4128-af1a-12e573c8fc0c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wrap-Up \n",
    "\n",
    "* Spark ML is a powerful tool to train ML models in a distributed way.\n",
    "* The syntax is similar to sklearn, but there are some differences.\n",
    "* Transformer and Estimator are the two main components in a fitting process. Transformers transform data, Estimators fit models.\n",
    "* The feature values must first be transferred to a so-called feature vector.\n",
    "* The fitting process is parallelized, but the degree of parallelization depends on the model architecture.\n",
    "* The resulting model can be persisted and loaded again in any production environment.\n",
    "* Pipelines are a convenient way to chain several Transformers and Estimators together.\n",
    "* Once again, the principle of lazy behavior applies. The fitting process is only executed when the ```fit()``` method is called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1dc50",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congrats!\n",
    "\n",
    "You dived into ML via Pyspark.  \n",
    "ðŸš€ðŸš€ðŸš€\n",
    "\n",
    "Again, stop the local spark session.\n",
    "\n",
    "Then, let's move to the cloud! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70936572-3904-483f-ba42-54a97198e6cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4a2cc-6542-4017-b561-8ec343854c6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
