{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c7d155",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a model\n",
    "\n",
    "With the knowledge of some Spark and PySpark basics, one can move on to machine learning utilities: Spark ML (*) \n",
    "\n",
    "Here, we take a simple dataset and fit a regressor. This does not claim to be a complete data science process. More important is to get to know the analogies and differences to known frameworks such as scikit-learn.\n",
    "\n",
    "(*) Often, one hears the term *MLLib*. This usually refers to the *ML* library by Spark. Strictly speaking, these are two different things. Spark MLlib is the older of the two and is applied directly to RDDs. Spark ML, on the other hand, is built on top of DataFrames and provides a higher-level API that abstracts away some of the low-level details of building and tuning machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c38ffc0-5e0b-4f51-bf99-f4a5df9a163c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ceda32-eacb-4026-93cf-f0ac54b0ed82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/12 11:00:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session: SparkSession = SparkSession.builder.master(\"local\").appName(\"Local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3795bb1f-5d2b-4271-a784-0adc7ae0ec51",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc22d6b-bba4-40d6-a40d-ea5db08c899f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Iris, penguins, AirBnBs, Wines etc. You already know them by heart?   \n",
    "I know, i know, but they do their work. So let's go for housing prices. üèòÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4306a55e-8fe3-499c-9693-74cd72dda078",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark_session.read.csv(\"data/housing_wo_null.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab317b-e590-4620-92b7-29965d5cfe31",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## EDA\n",
    "\n",
    "Performing EDA using PySpark has its pros and cons. On the one hand, PySpark supports various statistical functions that can help you calculate summary statistics, identify outliers, and explore relationships between variables. This can be particularly useful when dealing with very large datasets that cannot be easily processed using other tools.\n",
    "\n",
    "On the other hand, PySpark does not support plotting and visualization, which can be an important part of the EDA process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4de50e0-6de3-44e5-b52e-02d80b567a5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54488a4-bae6-4465-bbef-1efaa3bdc14f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e1f56c-8791-4d62-b761-2e67f2f601ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------\n",
      " longitude          | -122.23  \n",
      " latitude           | 37.88    \n",
      " housing_median_age | 41.0     \n",
      " total_rooms        | 880.0    \n",
      " total_bedrooms     | 129.0    \n",
      " population         | 322.0    \n",
      " households         | 126.0    \n",
      " median_income      | 8.3252   \n",
      " median_house_value | 452600.0 \n",
      " ocean_proximity    | NEAR BAY \n",
      "-RECORD 1----------------------\n",
      " longitude          | -122.22  \n",
      " latitude           | 37.86    \n",
      " housing_median_age | 21.0     \n",
      " total_rooms        | 7099.0   \n",
      " total_bedrooms     | 1106.0   \n",
      " population         | 2401.0   \n",
      " households         | 1138.0   \n",
      " median_income      | 8.3014   \n",
      " median_house_value | 358500.0 \n",
      " ocean_proximity    | NEAR BAY \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0f05d7-7960-4085-84aa-ba57b1a30529",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f663f77d-f298-4119-aba7-f94438585b88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>longitude</th><th>latitude</th><th>housing_median_age</th><th>total_rooms</th><th>total_bedrooms</th><th>population</th><th>households</th><th>median_income</th><th>median_house_value</th><th>ocean_proximity</th></tr>\n",
       "<tr><td>-122.23</td><td>37.88</td><td>41.0</td><td>880.0</td><td>129.0</td><td>322.0</td><td>126.0</td><td>8.3252</td><td>452600.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.22</td><td>37.86</td><td>21.0</td><td>7099.0</td><td>1106.0</td><td>2401.0</td><td>1138.0</td><td>8.3014</td><td>358500.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.24</td><td>37.85</td><td>52.0</td><td>1467.0</td><td>190.0</td><td>496.0</td><td>177.0</td><td>7.2574</td><td>352100.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>1274.0</td><td>235.0</td><td>558.0</td><td>219.0</td><td>5.6431</td><td>341300.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>1627.0</td><td>280.0</td><td>565.0</td><td>259.0</td><td>3.8462</td><td>342200.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>919.0</td><td>213.0</td><td>413.0</td><td>193.0</td><td>4.0368</td><td>269700.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.84</td><td>52.0</td><td>2535.0</td><td>489.0</td><td>1094.0</td><td>514.0</td><td>3.6591</td><td>299200.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.84</td><td>52.0</td><td>3104.0</td><td>687.0</td><td>1157.0</td><td>647.0</td><td>3.12</td><td>241400.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.84</td><td>42.0</td><td>2555.0</td><td>665.0</td><td>1206.0</td><td>595.0</td><td>2.0804</td><td>226700.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.25</td><td>37.84</td><td>52.0</td><td>3549.0</td><td>707.0</td><td>1551.0</td><td>714.0</td><td>3.6912</td><td>261100.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.85</td><td>52.0</td><td>2202.0</td><td>434.0</td><td>910.0</td><td>402.0</td><td>3.2031</td><td>281500.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.85</td><td>52.0</td><td>3503.0</td><td>752.0</td><td>1504.0</td><td>734.0</td><td>3.2705</td><td>241800.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.85</td><td>52.0</td><td>2491.0</td><td>474.0</td><td>1098.0</td><td>468.0</td><td>3.075</td><td>213500.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.84</td><td>52.0</td><td>696.0</td><td>191.0</td><td>345.0</td><td>174.0</td><td>2.6736</td><td>191300.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.85</td><td>52.0</td><td>2643.0</td><td>626.0</td><td>1212.0</td><td>620.0</td><td>1.9167</td><td>159200.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.85</td><td>50.0</td><td>1120.0</td><td>283.0</td><td>697.0</td><td>264.0</td><td>2.125</td><td>140000.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.27</td><td>37.85</td><td>52.0</td><td>1966.0</td><td>347.0</td><td>793.0</td><td>331.0</td><td>2.775</td><td>152500.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.27</td><td>37.85</td><td>52.0</td><td>1228.0</td><td>293.0</td><td>648.0</td><td>303.0</td><td>2.1202</td><td>155500.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.26</td><td>37.84</td><td>50.0</td><td>2239.0</td><td>455.0</td><td>990.0</td><td>419.0</td><td>1.9911</td><td>158700.0</td><td>NEAR BAY</td></tr>\n",
       "<tr><td>-122.27</td><td>37.84</td><td>52.0</td><td>1503.0</td><td>298.0</td><td>690.0</td><td>275.0</td><td>2.6033</td><td>162900.0</td><td>NEAR BAY</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
       "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
       "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
       "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
       "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
       "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
       "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
       "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
       "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n",
       "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n",
       "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n",
       "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n",
       "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n",
       "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n",
       "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n",
       "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n",
       "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n",
       "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n",
       "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n",
       "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n",
       "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n",
       "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n",
       "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n",
       "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0eb77c-148e-420d-8ec3-a24b1fcd3e25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mean = df.select([mean(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b31dd7-f49e-4f3c-82f6-de2adc04ecb1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57528d8d-37c1-4ec6-8f22-7f2eda220a58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_std = df.select([stddev(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f49e5-6b7e-4030-87aa-10a42b71b167",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eceac2d-2f3d-45d9-9856-7eca361547b4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build a model...\n",
    "\n",
    "Let's build a simple regression model which aims to predict the *median house value*.\n",
    "\n",
    "But first, let's clarify [terminology](https://spark.apache.org/docs/latest/ml-pipeline.html#example-estimator-transformer-and-param) for a moment:\n",
    "\n",
    "* We already know **DataFrames**.\n",
    "* **Transformers** can transform one DataFrame to another DataFrame. An ML Transformer transforms a DataFrame with features to DataFrame with predictions.\n",
    "* **Estimators** create Transformers via Fitting on a DataFrame.\n",
    "* A **Pipeline** chains Transformers and Estimators to create a Pipeline workflow. But they are not covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b5b66d9-5e3f-4285-bfec-591f028728a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"longitude\", \n",
    "            \"latitude\", \n",
    "            \"housing_median_age\", \n",
    "            \"total_rooms\", \n",
    "            \"total_bedrooms\", \n",
    "            \"population\",\n",
    "            \"households\",\n",
    "            \"median_income\"\n",
    "           ]\n",
    "TARGET = \"median_house_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ea01-f23f-47bd-8d9e-c44e436428ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most machine learning algorithms in Spark require input data to be in vector format. PySpark's VectorAssembler is a utility that allows you to combine multiple columns of a PySpark DataFrame into a single vector column. The resulting vector column is then used as an input to machine learning algorithms in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8df05cb-99b6-4040-a07a-9599f52dc9ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURES,\n",
    "    outputCol=\"features_vec\"\n",
    ")\n",
    "df_prep = assembler.transform(df).select(\"features_vec\", TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fcd51d1-29cb-468a-ae92-e4461289edb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|        features_vec|median_house_value|\n",
      "+--------------------+------------------+\n",
      "|[-122.23,37.88,41...|          452600.0|\n",
      "|[-122.22,37.86,21...|          358500.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_prep.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6152e31-1732-421f-8ac6-b447a10c0851",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df_prep.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81019a-4953-4230-a62c-7a9cd456a925",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Allright. Let's fit the baseline model, which is a simple linear regression. Note that a linear regression model assumes it's features to be normal distributed. This is not the case here. But we will ignore this at this point, because it is not the focus of this notebook.\n",
    "\n",
    "The syntax is very similar to sklearn. But, as mentioned above, there is a distingtion between Estimators and Transformers. So, the fitting process returns a Transformer object (in contrast to sklearn, where the object fitted object is able to make predictions directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e9f7f8-5e41-428c-885d-3d4145b04520",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol=TARGET, featuresCol=\"features_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9e24bc-9ed0-429b-9b5b-8ac4353a70e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 11:06:07 WARN Instrumentation: [1746f8b0] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/12 11:06:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/12 11:06:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "model = lr.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bc3fe-d498-4e63-a945-d21573684596",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The question whether and how the training process can now be parallelized depends on the model architecture. A knn clustering, for example, can be easily parallelized. An xgboost method, on the other hand, is somewhat more complex. Since the training process is sequential, only the individual steps can be parallelized here. The way in which the calculation is optimized differs in each case. This also applies to the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab8fa87e-e2eb-470b-a149-ab55d2be917f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f6b91-e133-4215-b741-0be673eb6cbb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ... test it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe94a84-d252-4ac6-bc2f-47456c96c6fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|        features_vec|median_house_value|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|[-124.3,41.84,17....|          103600.0|102138.32319061132|\n",
      "|[-124.23,40.54,52...|          106700.0|190038.23036733596|\n",
      "|[-124.23,41.75,11...|           73200.0| 77197.94015233079|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae2de6bb-bb33-4a10-a7dd-27306ee45f01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4856128708.42\n",
      "MAE: 51153.47\n",
      "RMSE: 69685.93\n",
      "R2: 0.63\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=TARGET, predictionCol='prediction')\n",
    "mse = evaluator.evaluate(df_pred, {evaluator.metricName: 'mse'})\n",
    "mae = evaluator.evaluate(df_pred, {evaluator.metricName: 'mae'})\n",
    "rmse = evaluator.evaluate(df_pred, {evaluator.metricName: 'rmse'})\n",
    "r2 = evaluator.evaluate(df_pred, {evaluator.metricName: 'r2'})\n",
    "\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd35522-19e7-4bf5-bff7-16973137650f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ... and persist it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90316fcb-a626-4710-b4e8-9c8cb6c4cf68",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So here we are. We've got a regressor in our hands. Hooray.\n",
    "\n",
    "For sure, we don't want to train a model again and again but persist it. The Transformer object comes with an built in function to do so. One can persist the model on localhost or any blob storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda7ae03-4e03-4e65-aca1-84b0ec53b1d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc6669e9-c6f9-429b-8e2a-4cb78467e288",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"model/my_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfe6ef97-d0ca-4e88-843e-1b076d5c203a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(path):\n",
    "    os.system(f'rm -r {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30a7b070-77d4-4824-9c19-6f7f8be34ddc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6eab1-fbf7-4b2a-8056-3f904e0cf32f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Then, in any production environment, one can load the model to work with it. Note that this could also happen in Scala or Java directly, because the serialization was not done via pickle but is language-agnostic.\n",
    "Again, one faces some unintuitive syntax, because the ```load()``` method is implemented in a Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30272a6a-5353-46d9-8a6e-df3837029d82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_loaded = LinearRegressionModel.load(\"model/my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e06cd2-733f-4cc6-8e04-9de3d4916136",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Chain in a pipeline\n",
    "\n",
    "There is also the possibility to capture these two steps directly in one object. Pipelines form a construct in which several Transformers and Estimators can be chained together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dd8abdd-6ee0-4c5a-af53-18e0ea5d3748",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURES,\n",
    "    outputCol=\"features_vec\"\n",
    ")\n",
    "\n",
    "lr = LinearRegression(labelCol=TARGET, featuresCol=\"features_vec\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8158f8-4f08-440c-92b3-f0bc5bbcc8ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case, the train-test-split has to be applied directly on the original DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56ebc95c-25e8-404a-bcd6-87cad5116a56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_direct, df_test_direct = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87af90f4-1fef-4030-a642-000db317c492",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 11:10:24 WARN Instrumentation: [7746911c] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df_train_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beefa734-b607-4c2e-a263-b873792b18d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = model.transform(df_test_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a525f031-ea6a-490a-8427-e9ebd6fe0827",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|        features_vec|        prediction|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+------------------+\n",
      "|   -124.3|   41.84|              17.0|     2677.0|         531.0|    1244.0|     456.0|       3.0313|          103600.0|     NEAR OCEAN|[-124.3,41.84,17....|102138.32319061132|\n",
      "|  -124.23|   40.54|              52.0|     2694.0|         453.0|    1152.0|     435.0|       3.0806|          106700.0|     NEAR OCEAN|[-124.23,40.54,52...|190038.23036733596|\n",
      "|  -124.23|   41.75|              11.0|     3159.0|         616.0|    1343.0|     479.0|       2.4805|           73200.0|     NEAR OCEAN|[-124.23,41.75,11...| 77197.94015233079|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e94e88-e24f-4975-8d9a-6d707215e9f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "Note that the example above is for education purposes. In real world problems, one might try out different hyperparameter setups as well as different models to reach the best one. Also, cross validation should be applied if possible to acchieve more stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dcf8995-50ed-4a5a-8dff-9e27abd4d56c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURES,\n",
    "    outputCol=\"features_vec\"\n",
    ")\n",
    "\n",
    "lr = LinearRegression(labelCol=TARGET, featuresCol=\"features_vec\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=TARGET, \n",
    "                                predictionCol='prediction',\n",
    "                                metricName=\"mae\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "parameter_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=parameter_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2\n",
    ")\n",
    "\n",
    "cross_val_model = crossval.fit(df_train_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadf51a-71a5-4950-a1a7-14731c2f00c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEEP_DIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da569dc-19a3-440e-a2a3-8c998a247f7b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEEP_DIVE: \n",
    "    print(cross_val_model.bestModel.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee8871-1764-437a-aeee-fc35fd11d727",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEEP_DIVE:\n",
    "    print(cross_val_model.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad3fca-8f20-4811-9c54-b25ddf741eb8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Outlook\n",
    "\n",
    "Real data sets usually require some pre-processing before fitting is possible.\n",
    "\n",
    "* For instance, not every ML model is able to handle missing values. One needs to drop oder impute missing values first. \n",
    "* Also, some models are sensitive to different scales in the expressions of numerical values, so one needs to standardize.\n",
    "* Speaking about numerical values, for many models categorical data have to be encoded first. \n",
    "\n",
    "For those cases, there exist Transformers as well. One should integrate them in the modelling process, best directly into a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de229d2b-315d-49da-a9bf-b2c67c5d2865",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congrats!\n",
    "\n",
    "You dived into ML via Pyspark.  \n",
    "üöÄüöÄüöÄ\n",
    "\n",
    "Again, stop the local spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70936572-3904-483f-ba42-54a97198e6cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4a2cc-6542-4017-b561-8ec343854c6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}